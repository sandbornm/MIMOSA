from os.path import join
import logging
import numpy as np
import copy

from comet_ml import Experiment

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split, SubsetRandomSampler
from torchsummary import summary

from sklearn.model_selection import RepeatedKFold

from learn.datasets import build_dataset
from learn.models import train_epoch, val_epoch, build_model

from util import makedir, get_parser

criterion = nn.BCELoss()


def cross_val(net,
          dataset,
          device,
          epochs,
          batch_size,
          lr,
          exp_name
          ):
    """
    Repeated K-Fold cross-validation for a PyTorch classifier
    """
    # comet setup
    experiment = Experiment(
        api_key="k86kE4n1wy7wQkkCmvZeFAV3M",
        project_name="mimosa",
        workspace="zstoebs",
    )
    experiment.set_name(exp_name)

    hyper_params = {
        "learning_rate": lr,
        "epochs": epochs,
        "batch_size": batch_size,
    }
    experiment.log_parameters(hyper_params)

    foldperf = {}
    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
    for fold, (train_idx, val_idx) in enumerate(cv.split(np.arange(len(dataset)))):

        logging.info('Fold {}'.format(fold + 1))

        train_sampler = SubsetRandomSampler(train_idx)
        val_sampler = SubsetRandomSampler(val_idx)
        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)

        model = copy.deepcopy(net)
        model.to(device)
        optimizer = torch.optim.Adam(net.parameters(), lr=lr)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)

        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

        for epoch in range(epochs):
            train_loss, train_metrics = train_epoch(model, device, train_loader, criterion, optimizer)
            val_loss, val_metrics = val_epoch(model, device, val_loader, criterion)

            train_loss /= len(train_sampler)
            val_loss /= len(val_sampler)

            with experiment.train():
                experiment.log_metric('Loss', train_loss, epoch=epoch)
                for metric, values in train_metrics.items():
                    experiment.log_metric(metric, np.mean(values), epoch=epoch)
            with experiment.validate():
                experiment.log_metric('Loss', val_loss, epoch=epoch)
                for metric, values in val_metrics.items():
                    experiment.log_metric(metric, np.mean(values), epoch=epoch)

            mean_train_acc = np.mean(train_metrics['accuracy'])
            mean_val_acc = np.mean(val_metrics['accuracy'])

            scheduler.step(mean_val_acc)

            logging.info(
                "Epoch:{}/{} \n"
                " AVG Training Loss:{:.3f} \n"
                "AVG Test Loss:{:.3f} \n"
                "AVG Training Acc {:.2f} \n"
                "AVG Test Acc {:.2f}".format(
                    epoch + 1,
                    epochs,
                    train_loss,
                    val_loss,
                    mean_train_acc,
                    mean_val_acc))

            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['train_acc'].append(mean_train_acc)
            history['val_acc'].append(mean_val_acc)

        foldperf['fold{}'.format(fold + 1)] = history


def train(net,
          dataset,
          device,
          epochs,
          batch_size,
          lr,
          val_percent,
          frequency,
          exp_name
          ):
    """
    Standard iterative training for a PyTorch classifier
    """

    # comet setup
    experiment = Experiment(
        api_key="k86kE4n1wy7wQkkCmvZeFAV3M",
        project_name="mimosa",
        workspace="zstoebs",
    )
    experiment.set_name(exp_name)

    save_dir = join('cp', exp_name)

    net.train()

    n_val = int(len(dataset) * val_percent)
    n_train = len(dataset) - n_val
    train, val = random_split(dataset, [n_train, n_val])
    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)
    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True,
                            drop_last=True)

    hyper_params = {
        "learning_rate": lr,
        "epochs": epochs,
        "batch_size": batch_size,
        "train_size": n_train,
        "val_size": n_val,
        "device": device.type,
    }
    experiment.log_parameters(hyper_params)

    logging.info(f'''Starting training:
        Epochs:          {epochs}
        Batch size:      {batch_size}
        Learning rate:   {lr}
        Training size:   {n_train}
        Validation size: {n_val}
        Device:          {device.type}
    ''')

    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)

    for epoch in range(epochs):
        train_kwargs = {'n_train': n_train, 'epoch': epoch, 'epochs': epochs}
        train_loss, train_metrics = train_epoch(net, device, train_loader, criterion, optimizer, **train_kwargs)
        val_loss, val_metrics = val_epoch(net, device, val_loader, criterion)

        train_loss /= n_train
        val_loss /= n_val

        with experiment.train():
            experiment.log_metric('Loss', train_loss, epoch=epoch)
            for metric, values in train_metrics.items():
                experiment.log_metric(metric, np.mean(values), epoch=epoch)
        with experiment.validate():
            experiment.log_metric('Loss', val_loss, epoch=epoch)
            for metric, values in val_metrics.items():
                experiment.log_metric(metric, np.mean(values), epoch=epoch)

        mean_train_acc = np.mean(train_metrics['accuracy'])
        mean_val_acc = np.mean(val_metrics['accuracy'])

        scheduler.step(mean_val_acc)

        logging.info(
            "Epoch:{}/{} \n "
            "AVG Training Loss:{:.3f} \n "
            "AVG Val Loss:{:.3f} \n "
            "AVG Training Acc {:.2f} \n "
            "AVG Val Acc {:.2f}".format(
                epoch + 1,
                epochs,
                train_loss,
                val_loss,
                mean_train_acc,
                mean_val_acc))

        # save checkpoint
        if frequency and epoch % frequency == 0:
            makedir(save_dir)
            torch.save(net.state_dict(),
                      join(save_dir, f'{exp_name}_epoch{epoch + 1}.pth'))
            logging.info(f'Checkpoint {epoch + 1} saved !')

    # save final model
    makedir(save_dir)
    torch.save(net.state_dict(),
               join(save_dir, f'{exp_name}_final.pth'))
    logging.info(f'Final model saved !')


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    parser = get_parser()
    args = parser.parse_args()

    # create dataset
    dataset = build_dataset(args)
    n_examples = dataset.n_examples
    n_classes = dataset.n_classes

    # build net
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    net, criterion = build_model(args, n_classes)

    input_sz = dataset[0]['example'].shape
    logging.info(f'Dataset: # of examples = {n_examples}, # of classes = {n_classes}, input size = {input_sz}')

    summary(net, input_sz)
    logging.info(f'Using device {device}')

    # attempt load if spec'd
    if args.load:
        net.load_state_dict(
            torch.load(args.load, map_location=device)
        )
        logging.info(f'net loaded from {args.load}')

    net.to(device=device)

    if args.mode == 'train':
        train(net,
              dataset,
              device,
              args.epochs,
              args.batchsize,
              args.lr,
              args.val,
              args.frequency,
              args.name
              )
    elif args.mode == 'cv' or args.mode == 'cross_val':
        cross_val(net,
              dataset,
              device,
              args.epochs,
              args.batchsize,
              args.lr,
              args.name
              )
    else:
        logging.error('Unknown mode entered: %s' % args.mode)
