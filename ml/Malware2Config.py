from os.path import join
import argparse
import logging
import numpy as np
import copy
from tqdm import tqdm

from comet_ml import Experiment

import torch
from torch.utils.data import DataLoader, random_split, SubsetRandomSampler
from torchvision import transforms
from torchsummary import summary

from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from model.ImageMultilabelClassifier import ImageMultilabelClassifier, criterion
from dataloader.MalwareImageDataset import MalwareImageDataset

from util import makedir


def get_args():
    parser = argparse.ArgumentParser(description='Train a Malware2Config mulitlabel classifier')

    # meta setup
    parser.add_argument('--examples_dir', '-x', type=str, default="", help='path to image dir')
    parser.add_argument('--labels_csv', '-y', type=str, default="", help='path to labels csv')
    parser.add_argument('--image_sz', '-s', type=int, nargs='+', default=[64,64], help='resize images to these dims. format: --image_sz 64 64')
    parser.add_argument('--name', '-n', type=str, default='Malware2Config_Multilabel', help='Experiment name')
    parser.add_argument('--load', '-f', type=str, default='', help='path to net to load')
    parser.add_argument('--frequency', '-q', type=int, default=0, help='save frequency. default=end of training')

    # hyperparams
    parser.add_argument('-e', '--epochs', metavar='E', type=int, default=20, help='Number of epochs', dest='epochs')
    parser.add_argument('-b', '--batch_size', metavar='B', type=int, nargs='?', default=16, help='Batch size',
                        dest='batchsize')
    parser.add_argument('-l', '--learning-rate', metavar='LR', type=float, nargs='?', default=1e-3,
                        help='Learning rate', dest='lr')
    parser.add_argument('-v', '--val_percent', metavar='V', type=float, default=0.2, help='Percent for validation set', dest='val')


    return parser.parse_args()


# Use threshold to define predicted labels and invoke sklearn's metrics with different averaging strategies.
def calculate_metrics(pred, target, threshold=0.5):
    pred = np.array(pred > threshold, dtype=float)
    return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro', zero_division=1),
            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro', zero_division=1),
            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro', zero_division=1),
            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro', zero_division=1),
            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro', zero_division=1),
            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro', zero_division=1),
            'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples', zero_division=1),
            'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples', zero_division=1),
            'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples', zero_division=1),
            'accuracy': accuracy_score(y_true=target, y_pred=pred)
            }


def merge_dicts(dict_1, dict_2):
    """
    Merge dicts with common keys as list
    """
    dict_3 = {**dict_1, **dict_2}
    for key, value in dict_3.items():
        if key in dict_1 and key in dict_2:
            dict_3[key] = dict_1[key] + [value]  # since dict_1 val overwritten by above merge
    return dict_3


# https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f
def train_epoch(net, device, dataloader, loss_fn, optimizer, **kwargs):

    n_train = kwargs['n_train']
    epoch = kwargs['epoch']
    epochs = kwargs['epochs']

    train_loss = 0.0
    train_metrics = None
    net.train()
    with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:
        for batch in dataloader:
            images = batch['image']
            labels = batch['label']
            labels = torch.squeeze(labels).float()
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            output = net(images)
            loss = loss_fn(output, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * images.size(0)

            metrics = calculate_metrics(output.detach().cpu(), labels.detach().cpu().numpy())
            train_metrics = metrics if not train_metrics else merge_dicts(train_metrics, metrics)

            pbar.update(images.shape[0])

    return train_loss, train_metrics


def valid_epoch(net, device, dataloader, loss_fn):
    val_loss = 0.0
    val_metrics = None
    net.eval()
    for batch in dataloader:
        images = batch['image']
        labels = batch['label']
        labels = torch.squeeze(labels).float()
        images, labels = images.to(device), labels.to(device)
        output = net(images)
        loss = loss_fn(output, labels)
        val_loss += loss.item() * images.size(0)

        metrics = calculate_metrics(output.detach().cpu(), labels.detach().cpu().numpy())
        val_metrics = metrics if not val_metrics else merge_dicts(val_metrics, metrics)

    return val_loss, val_metrics


def cross_val(net,
          dataset,
          device,
          epochs,
          batch_size,
          lr,
          exp_name
          ):

    # comet setup
    experiment = Experiment(
        api_key="k86kE4n1wy7wQkkCmvZeFAV3M",
        project_name="mimosa",
        workspace="zstoebs",
    )
    experiment.set_name(exp_name)

    hyper_params = {
        "learning_rate": lr,
        "epochs": epochs,
        "batch_size": batch_size,
    }
    experiment.log_parameters(hyper_params)

    foldperf = {}
    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
    for fold, (train_idx, val_idx) in enumerate(cv.split(np.arange(len(dataset)))):

        logging.info('Fold {}'.format(fold + 1))

        train_sampler = SubsetRandomSampler(train_idx)
        val_sampler = SubsetRandomSampler(val_idx)
        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)

        model = copy.deepcopy(net)
        model.to(device)
        optimizer = torch.optim.Adam(net.parameters(), lr=lr)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)

        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

        for epoch in range(epochs):
            train_loss, train_metrics = train_epoch(model, device, train_loader, criterion, optimizer)
            val_loss, val_metrics = valid_epoch(model, device, val_loader, criterion)

            train_loss /= len(train_sampler)
            val_loss /= len(val_sampler)

            with experiment.train():
                experiment.log_metric('Train Loss', train_loss, epoch=epoch)
                for metric, values in train_metrics.items():
                    experiment.log_metric(metric, np.mean(values), epoch=epoch)
            with experiment.validate():
                experiment.log_metric('Val Loss', val_loss, epoch=epoch)
                for metric, values in val_metrics.items():
                    experiment.log_metric(metric, np.mean(values), epoch=epoch)

            mean_train_acc = np.mean(train_metrics['accuracy'])
            mean_val_acc = np.mean(val_metrics['accuracy'])

            scheduler.step(mean_val_acc)

            logging.info(
                "Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %".format(
                    epoch + 1,
                    epochs,
                    train_loss,
                    val_loss,
                    mean_train_acc,
                    mean_val_acc))

            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['train_acc'].append(mean_train_acc)
            history['val_acc'].append(mean_val_acc)

        foldperf['fold{}'.format(fold + 1)] = history


def train(net,
          dataset,
          device,
          epochs,
          batch_size,
          lr,
          val_percent,
          frequency,
          exp_name
          ):

    # comet setup
    experiment = Experiment(
        api_key="k86kE4n1wy7wQkkCmvZeFAV3M",
        project_name="mimosa",
        workspace="zstoebs",
    )
    experiment.set_name(exp_name)

    save_dir = join('cp', exp_name)

    hyper_params = {
        "learning_rate": lr,
        "epochs": epochs,
        "batch_size": batch_size,
    }
    experiment.log_parameters(hyper_params)

    with experiment.train():
        net.train()

        n_val = int(len(dataset) * val_percent)
        n_train = len(dataset) - n_val
        train, val = random_split(dataset, [n_train, n_val])
        train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)
        val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True,
                                drop_last=True)

        logging.info(f'''Starting training:
            Epochs:          {epochs}
            Batch size:      {batch_size}
            Learning rate:   {lr}
            Training size:   {n_train}
            Validation size: {n_val}
            Device:          {device.type}
        ''')

        optimizer = torch.optim.Adam(net.parameters(), lr=lr)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)

        for epoch in range(epochs):
            train_args = {'n_train': n_train, 'epoch': epoch, 'epochs': epochs}
            train_loss, train_metrics = train_epoch(net, device, train_loader, criterion, optimizer, **train_args)
            val_loss, val_metrics = valid_epoch(net, device, val_loader, criterion)

            train_loss /= n_train
            val_loss /= n_val

            with experiment.train():
                experiment.log_metric('Train Loss', train_loss, epoch=epoch)
                for metric, values in train_metrics.items():
                    experiment.log_metric(metric, np.mean(values), epoch=epoch)
            with experiment.validate():
                experiment.log_metric('Val Loss', val_loss, epoch=epoch)
                for metric, values in val_metrics.items():
                    experiment.log_metric(metric, np.mean(values), epoch=epoch)

            mean_train_acc = np.mean(train_metrics['accuracy'])
            mean_val_acc = np.mean(val_metrics['accuracy'])

            scheduler.step(mean_val_acc)

            logging.info(
                "Epoch:{}/{} \n "
                "AVG Training Loss:{:.3f} \n "
                "AVG Val Loss:{:.3f} \n "
                "AVG Training Acc {:.2f} % \n "
                "AVG Val Acc {:.2f} %".format(
                    epoch + 1,
                    epochs,
                    train_loss,
                    val_loss,
                    mean_train_acc,
                    mean_val_acc))

            # save checkpoint
            if frequency and epoch % frequency == 0:
                makedir(save_dir)
                torch.save(net.state_dict(),
                          join(save_dir, f'{exp_name}_epoch{epoch + 1}.pth'))
                logging.info(f'Checkpoint {epoch + 1} saved !')

    # save final model
    makedir(save_dir)
    torch.save(net.state_dict(),
               join(save_dir, f'{exp_name}_final.pth'))
    logging.info(f'Final model saved !')



if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    args = get_args()

    # create dataset
    transform = transforms.Compose([transforms.ToTensor()])
    dataset = MalwareImageDataset(args.examples_dir, args.labels_csv, transform=transform, image_sz=tuple(args.image_sz))
    n_examples = dataset.n_examples
    n_classes = dataset.n_classes
    logging.info('Dataset: # of examples = %d, # of classes = %d' % (n_examples, n_classes))

    # build net
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    net = ImageMultilabelClassifier(n_classes)

    summary(net, dataset[0]['image'].shape)
    logging.info(f'Using device {device}')

    # attempt load if spec'd
    if args.load:
        net.load_state_dict(
            torch.load(args.load, map_location=device)
        )
        logging.info(f'net loaded from {args.load}')

    net.to(device=device)

    train(net,
          dataset,
          device,
          args.epochs,
          args.batchsize,
          args.lr,
          args.val,
          args.frequency,
          args.name
          )


